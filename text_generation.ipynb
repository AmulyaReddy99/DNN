{"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"text_generation.ipynb","private_outputs":true,"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np, os, time","metadata":{"colab":{},"colab_type":"code","id":"yG_n40gFzf9s"},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')","metadata":{"colab":{},"colab_type":"code","id":"pD_55cOxLkAb"},"execution_count":3,"outputs":[{"name":"stdout","output_type":"stream","text":"Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n1122304/1115394 [==============================] - 0s 0us/step\n"}]},{"cell_type":"code","source":"text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\nprint ('Length of text: {} characters'.format(len(text)))","metadata":{"colab":{},"colab_type":"code","id":"aavnuByVymwK"},"execution_count":4,"outputs":[{"name":"stdout","output_type":"stream","text":"Length of text: 1115394 characters\n"}]},{"cell_type":"code","source":"print(text[:250])","metadata":{"colab":{},"colab_type":"code","id":"Duhg9NrUymwO"},"execution_count":5,"outputs":[{"name":"stdout","output_type":"stream","text":"First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\n"}]},{"cell_type":"code","source":"vocab = sorted(set(text))\nprint ('{} unique characters'.format(len(vocab)))","metadata":{"colab":{},"colab_type":"code","id":"IlCgQBRVymwR"},"execution_count":6,"outputs":[{"name":"stdout","output_type":"stream","text":"65 unique characters\n"}]},{"cell_type":"code","source":"char2idx = {u:i for i, u in enumerate(vocab)}\nidx2char = np.array(vocab)\n\ntext_as_int = np.array([char2idx[c] for c in text])","metadata":{"colab":{},"colab_type":"code","id":"IalZLbvOzf-F"},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Now we have an integer representation for each character. Notice that we mapped the character as indexes from 0 to `len(unique)`.","metadata":{"colab_type":"text","id":"tZfqhkYCymwX"}},{"cell_type":"code","source":"print('{')\nfor char,_ in zip(char2idx, range(20)):\n    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]),endl=\" \")\nprint('  ...\\n}')","metadata":{"colab":{},"colab_type":"code","id":"FYyNlCNXymwY"},"execution_count":8,"outputs":[{"name":"stdout","output_type":"stream","text":"{\n  '\\n':   0,\n  ' ' :   1,\n  '!' :   2,\n  '$' :   3,\n  '&' :   4,\n  \"'\" :   5,\n  ',' :   6,\n  '-' :   7,\n  '.' :   8,\n  '3' :   9,\n  ':' :  10,\n  ';' :  11,\n  '?' :  12,\n  'A' :  13,\n  'B' :  14,\n  'C' :  15,\n  'D' :  16,\n  'E' :  17,\n  'F' :  18,\n  'G' :  19,\n  ...\n}\n"}]},{"cell_type":"code","source":"print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))","metadata":{"colab":{},"colab_type":"code","id":"l1VKcQHcymwb"},"execution_count":9,"outputs":[{"name":"stdout","output_type":"stream","text":"'First Citizen' ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"}]},{"cell_type":"code","source":"seq_length = 100\nexamples_per_epoch = len(text)//(seq_length+1)\n\nchar_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n\nfor i in char_dataset.take(5):\n  print(idx2char[i.numpy()])","metadata":{"colab":{},"colab_type":"code","id":"0UHJDA39zf-O"},"execution_count":10,"outputs":[{"name":"stdout","output_type":"stream","text":"F\ni\nr\ns\nt\n"}]},{"cell_type":"markdown","source":"The `batch` method lets us easily convert these individual characters to sequences of the desired size.","metadata":{"colab_type":"text","id":"-ZSYAcQV8OGP"}},{"cell_type":"code","source":"sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n\nfor item in sequences.take(5):\n  print(repr(''.join(idx2char[item.numpy()])))","metadata":{"colab":{},"colab_type":"code","id":"l4hkDU3i7ozi"},"execution_count":11,"outputs":[{"name":"stdout","output_type":"stream","text":"'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"}]},{"cell_type":"code","source":"def split_input_target(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\n\ndataset = sequences.map(split_input_target)","metadata":{"colab":{},"colab_type":"code","id":"9NGu-FkO_kYU"},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"for input_example, target_example in  dataset.take(1):\n  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))","metadata":{"colab":{},"colab_type":"code","id":"GNbw-iR0ymwj"},"execution_count":13,"outputs":[{"name":"stdout","output_type":"stream","text":"Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\nTarget data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"}]},{"cell_type":"code","source":"for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n    print(\"Step {:4d}\".format(i))\n    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))","metadata":{"colab":{},"colab_type":"code","id":"0eBu9WZG84i0"},"execution_count":14,"outputs":[{"name":"stdout","output_type":"stream","text":"Step    0\n  input: 18 ('F')\n  expected output: 47 ('i')\nStep    1\n  input: 47 ('i')\n  expected output: 56 ('r')\nStep    2\n  input: 56 ('r')\n  expected output: 57 ('s')\nStep    3\n  input: 57 ('s')\n  expected output: 58 ('t')\nStep    4\n  input: 58 ('t')\n  expected output: 1 (' ')\n"}]},{"cell_type":"code","source":"# Batch size\nBATCH_SIZE = 64\nBUFFER_SIZE = 10000\n\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\ndataset","metadata":{"colab":{},"colab_type":"code","id":"p2pGotuNzf-S"},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"},"metadata":{}}]},{"cell_type":"code","source":"vocab_size = len(vocab)\nembedding_dim = 256\nrnn_units = 1024","metadata":{"colab":{},"colab_type":"code","id":"zHT8cLh7EAsg"},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n  model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n                              batch_input_shape=[batch_size, None]),\n    tf.keras.layers.GRU(rnn_units,\n                        return_sequences=True,\n                        stateful=True,\n                        recurrent_initializer='glorot_uniform'),\n    tf.keras.layers.Dense(vocab_size)\n  ])\n  return model","metadata":{"colab":{},"colab_type":"code","id":"MtCrdfzEI2N0"},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"model = build_model(\n  vocab_size = len(vocab),\n  embedding_dim=embedding_dim,\n  rnn_units=rnn_units,\n  batch_size=BATCH_SIZE)","metadata":{"colab":{},"colab_type":"code","id":"wwsrpOik5zhv"},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"for input_example_batch, target_example_batch in dataset.take(1):\n    example_batch_predictions = model(input_example_batch)\n    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")","metadata":{"colab":{},"colab_type":"code","id":"C-_70kKAPrPU"},"execution_count":19,"outputs":[{"name":"stdout","output_type":"stream","text":"(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"}]},{"cell_type":"code","source":"model.summary()","metadata":{"colab":{},"colab_type":"code","id":"vPGmAAXmVLGC"},"execution_count":20,"outputs":[{"name":"stdout","output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (64, None, 256)           16640     \n_________________________________________________________________\ngru (GRU)                    (64, None, 1024)          3938304   \n_________________________________________________________________\ndense (Dense)                (64, None, 65)            66625     \n=================================================================\nTotal params: 4,021,569\nTrainable params: 4,021,569\nNon-trainable params: 0\n_________________________________________________________________\n"}]},{"cell_type":"code","source":"sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\nsampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()","metadata":{"colab":{},"colab_type":"code","id":"4V4MfFg0RQJg"},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"sampled_indices","metadata":{"colab":{},"colab_type":"code","id":"YqFMUQc_UFgM"},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"array([51, 23, 37,  5, 46, 55, 22,  1,  0,  3, 17, 38, 62, 25, 52, 57, 49,\n       49, 53, 54, 41, 25, 63, 24, 16, 48, 28, 61, 54, 28, 35,  1, 50, 39,\n        9, 52, 34, 32, 40, 48,  5,  0,  0, 47, 56,  4, 24, 43, 46, 32, 42,\n       28, 22, 40, 22, 49, 34, 16, 12, 51, 39, 40, 41,  0, 50,  0, 11, 56,\n       60, 54, 11, 60, 38,  0, 19, 59,  7, 20, 17, 36, 12, 14,  3, 14, 61,\n       49, 19, 31, 49, 36, 41, 41, 56, 29, 57, 31, 49, 58, 55, 16])"},"metadata":{}}]},{"cell_type":"code","source":"print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\nprint()\nprint(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))","metadata":{"colab":{},"colab_type":"code","id":"xWcFwPwLSo05"},"execution_count":23,"outputs":[{"name":"stdout","output_type":"stream","text":"Input: \n \"I'll groan, the way being short,\\nAnd piece the way out with a heavy heart.\\nCome, come, in wooing sor\"\n\nNext Char Predictions: \n \"mKY'hqJ \\n$EZxMnskkopcMyLDjPwpPW la3nVTbj'\\n\\nir&LehTdPJbJkVD?mabc\\nl\\n;rvp;vZ\\nGu-HEX?B$BwkGSkXccrQsSktqD\"\n"}]},{"cell_type":"code","source":"def loss(labels, logits):\n    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n\nexample_batch_loss  = loss(target_example_batch, example_batch_predictions)\nprint(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\nprint(\"scalar_loss:      \", example_batch_loss.numpy().mean())","metadata":{"colab":{},"colab_type":"code","id":"4HrXTACTdzY-"},"execution_count":24,"outputs":[{"name":"stdout","output_type":"stream","text":"Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\nscalar_loss:       4.1748996\n"}]},{"cell_type":"code","source":"model.compile(optimizer='adam', loss=loss)","metadata":{"colab":{},"colab_type":"code","id":"DDl1_Een6rL0"},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"checkpoint_dir = './training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)","metadata":{"colab":{},"colab_type":"code","id":"W6fWTriUZP-n"},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"EPOCHS=10","metadata":{"colab":{},"colab_type":"code","id":"7yGBE2zxMMHs"},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])","metadata":{"colab":{},"colab_type":"code","id":"UK-hmKjYVoll"},"execution_count":28,"outputs":[{"name":"stdout","output_type":"stream","text":"Train for 172 steps\nEpoch 1/10\n172/172 [==============================] - 7s 40ms/step - loss: 2.6448\nEpoch 2/10\n172/172 [==============================] - 6s 35ms/step - loss: 1.9463\nEpoch 3/10\n172/172 [==============================] - 6s 34ms/step - loss: 1.6828\nEpoch 4/10\n172/172 [==============================] - 6s 34ms/step - loss: 1.5384\nEpoch 5/10\n172/172 [==============================] - 6s 34ms/step - loss: 1.4516\nEpoch 6/10\n172/172 [==============================] - 6s 34ms/step - loss: 1.3929\nEpoch 7/10\n172/172 [==============================] - 6s 33ms/step - loss: 1.3476\nEpoch 8/10\n172/172 [==============================] - 6s 34ms/step - loss: 1.3103\nEpoch 9/10\n172/172 [==============================] - 6s 36ms/step - loss: 1.2751\nEpoch 10/10\n172/172 [==============================] - 6s 34ms/step - loss: 1.2434\n"}]},{"cell_type":"code","source":"tf.train.latest_checkpoint(checkpoint_dir)","metadata":{"colab":{},"colab_type":"code","id":"zk2WJ2-XjkGz"},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"'./training_checkpoints/ckpt_10'"},"metadata":{}}]},{"cell_type":"code","source":"model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n\nmodel.build(tf.TensorShape([1, None]))","metadata":{"colab":{},"colab_type":"code","id":"LycQ-ot_jjyu"},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"colab":{},"colab_type":"code","id":"71xa6jnYVrAN"},"execution_count":31,"outputs":[{"name":"stdout","output_type":"stream","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (1, None, 256)            16640     \n_________________________________________________________________\ngru_1 (GRU)                  (1, None, 1024)           3938304   \n_________________________________________________________________\ndense_1 (Dense)              (1, None, 65)             66625     \n=================================================================\nTotal params: 4,021,569\nTrainable params: 4,021,569\nNon-trainable params: 0\n_________________________________________________________________\n"}]},{"cell_type":"code","source":"def generate_text(model, start_string):\n    num_generate = 1000\n    input_eval = [char2idx[s] for s in start_string]\n    input_eval = tf.expand_dims(input_eval, 0)\n    text_generated = []\n    temperature = 1.0\n\n    model.reset_states()\n    for i in range(num_generate):\n        predictions = model(input_eval)\n        predictions = tf.squeeze(predictions, 0)\n\n        predictions = predictions / temperature\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n        input_eval = tf.expand_dims([predicted_id], 0)\n\n        text_generated.append(idx2char[predicted_id])\n\n    return (start_string + ''.join(text_generated))","metadata":{"colab":{},"colab_type":"code","id":"WvuwZBX5Ogfd"},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"print(generate_text(model, start_string=u\"ROMEO: \"))","metadata":{"colab":{},"colab_type":"code","id":"ktovv0RFhrkn"},"execution_count":33,"outputs":[{"name":"stdout","output_type":"stream","text":"ROMEO: I am unplume, shalt to the\nFrancies would women cluncime against it.\n\nMENENIUS:\nPemprescorce that you shall not be thus; let's in justices and by\nhave thoughts, to test the stars as great\nAs open lack gawned raged\nDuke of Northumberland, this manner of his prince.\n\nMARCIUS:\nHow do thou wast forced;\nThe endempily east enought than whence, or, bear\nheaded me aple, to-morrow why I rue,\nMy own brothers on't, but stins abbooon of\nso sours; or ghinf purnicy in base as as\nTwo kings at my heart?\n\nDUKE OF AUMERLE:\nWhich dost thou be in this person? peace: but whoreson hat a not flee\nWhose honour and athe stampet's presence in sorrow is there?\n\nBIONDELLO:\nWhat can you gone, and suck'd your prunishen.\nThey, we beseech you, sister, whilst Margaret pleasant, gentle speed?\nAbout! following proclaim the end,\nAnd whom unto his rooate passion,\nGo apperial court.\nNow, sir, but 'tain for Richard, whom is that all.\n\nPETRUCHIO:\nNow, there, i' for the !\nWith welt repared on Rome, to\nGrimy, sad I fought. Whe\n"}]},{"cell_type":"markdown","source":"The easiest thing you can do to improve the results it to train it for longer (try `EPOCHS=30`).\n\nYou can also experiment with a different start string, or try adding another RNN layer to improve the model's accuracy, or adjusting the temperature parameter to generate more or less random predictions.","metadata":{"colab_type":"text","id":"AM2Uma_-yVIq"}},{"cell_type":"code","source":"model = build_model(\n  vocab_size = len(vocab),\n  embedding_dim=embedding_dim,\n  rnn_units=rnn_units,\n  batch_size=BATCH_SIZE)","metadata":{"colab":{},"colab_type":"code","id":"_XAm7eCoKULT"},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam()","metadata":{"colab":{},"colab_type":"code","id":"qUKhnZtMVpoJ"},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef train_step(inp, target):\n    with tf.GradientTape() as tape:\n    predictions = model(inp)\n    loss = tf.reduce_mean(\n        tf.keras.losses.sparse_categorical_crossentropy(\n            target, predictions, from_logits=True))\n    grads = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n    return loss","metadata":{"colab":{},"colab_type":"code","id":"b4kH1o0leVIp"},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 10\n\nfor epoch in range(EPOCHS):\n    start = time.time()\n    hidden = model.reset_states()\n\n    for (batch_n, (inp, target)) in enumerate(dataset):\n    loss = train_step(inp, target)\n\n    if batch_n % 100 == 0:\n        template = 'Epoch {} Batch {} Loss {}'\n        print(template.format(epoch+1, batch_n, loss))\n\n    if (epoch + 1) % 5 == 0:\n    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n\n    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n\n    model.save_weights(checkpoint_prefix.format(epoch=epoch))","metadata":{"colab":{},"colab_type":"code","id":"d4tSNwymzf-q"},"execution_count":37,"outputs":[{"name":"stdout","output_type":"stream","text":"Epoch 1 Batch 0 Loss 4.175037384033203\nEpoch 1 Batch 100 Loss 2.369581699371338\nEpoch 1 Loss 2.1165\nTime taken for 1 epoch 6.417382717132568 sec\n\nEpoch 2 Batch 0 Loss 2.1537587642669678\nEpoch 2 Batch 100 Loss 1.9563190937042236\nEpoch 2 Loss 1.8062\nTime taken for 1 epoch 5.329235792160034 sec\n\nEpoch 3 Batch 0 Loss 1.8056563138961792\nEpoch 3 Batch 100 Loss 1.7106741666793823\nEpoch 3 Loss 1.6115\nTime taken for 1 epoch 5.339670419692993 sec\n\nEpoch 4 Batch 0 Loss 1.5668939352035522\nEpoch 4 Batch 100 Loss 1.5268672704696655\nEpoch 4 Loss 1.4890\nTime taken for 1 epoch 5.412369728088379 sec\n\nEpoch 5 Batch 0 Loss 1.4938063621520996\nEpoch 5 Batch 100 Loss 1.4473059177398682\nEpoch 5 Loss 1.4000\nTime taken for 1 epoch 5.519220590591431 sec\n\nEpoch 6 Batch 0 Loss 1.3862831592559814\nEpoch 6 Batch 100 Loss 1.410801887512207\nEpoch 6 Loss 1.3671\nTime taken for 1 epoch 5.347445011138916 sec\n\nEpoch 7 Batch 0 Loss 1.346337080001831\nEpoch 7 Batch 100 Loss 1.3436977863311768\nEpoch 7 Loss 1.3429\nTime taken for 1 epoch 5.457265853881836 sec\n\nEpoch 8 Batch 0 Loss 1.3194979429244995\nEpoch 8 Batch 100 Loss 1.3179987668991089\nEpoch 8 Loss 1.3036\nTime taken for 1 epoch 5.361689329147339 sec\n\nEpoch 9 Batch 0 Loss 1.253419041633606\nEpoch 9 Batch 100 Loss 1.2753329277038574\nEpoch 9 Loss 1.2893\nTime taken for 1 epoch 5.390798807144165 sec\n\nEpoch 10 Batch 0 Loss 1.1887139081954956\nEpoch 10 Batch 100 Loss 1.2455520629882812\nEpoch 10 Loss 1.2771\nTime taken for 1 epoch 5.569040298461914 sec\n\n"}]}]}